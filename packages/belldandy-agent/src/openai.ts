import type { JsonObject } from "@belldandy/protocol";

import type { AgentRunInput, AgentStreamItem, BelldandyAgent } from "./index.js";
import { FailoverClient, type ModelProfile, type FailoverLogger } from "./failover-client.js";
import { buildUrl, preprocessMultimodalContent, type VideoUploadConfig } from "./multimodal.js";

export type OpenAIChatAgentOptions = {
  baseUrl: string;
  apiKey: string;
  model: string;
  timeoutMs?: number;
  stream?: boolean;
  systemPrompt?: string;
  /** 备用 Profile 列表（模型容灾） */
  fallbacks?: ModelProfile[];
  /** 容灾日志接口 */
  failoverLogger?: FailoverLogger;
  /** 视频文件上传专用配置（当聊天代理不支持 /files 端点时） */
  videoUploadConfig?: VideoUploadConfig;
  /** 强制指定 API 协议（默认自动检测） */
  protocol?: ApiProtocol;
};

type ApiProtocol = "openai" | "anthropic";

/**
 * 检测 API 协议类型
 * - 如果指定了 forceProtocol 参数，优先使用该值
 * - 否则根据 baseUrl 自动检测
 * - Anthropic 协议：Anthropic 官方、Kimi Code 等
 * - OpenAI 协议：OpenAI、Azure、Moonshot、Gemini 等
 */
function detectProtocol(baseUrl: string, forceProtocol?: ApiProtocol): ApiProtocol {
  // 优先使用强制指定的协议
  if (forceProtocol) {
    return forceProtocol;
  }

  const lowerUrl = baseUrl.toLowerCase();
  // Anthropic 官方 API 或兼容端点
  if (lowerUrl.includes("anthropic.com") || lowerUrl.includes("kimi.com/coding")) {
    return "anthropic";
  }
  return "openai";
}

export class OpenAIChatAgent implements BelldandyAgent {
  private readonly opts: Required<Pick<OpenAIChatAgentOptions, "timeoutMs" | "stream">> &
    Omit<OpenAIChatAgentOptions, "timeoutMs" | "stream">;
  private readonly failoverClient: FailoverClient;
  private readonly protocol: ApiProtocol;

  constructor(opts: OpenAIChatAgentOptions) {
    this.opts = {
      ...opts,
      timeoutMs: opts.timeoutMs ?? 60_000,
      stream: opts.stream ?? true,
    };

    // 检测 API 协议类型（优先使用用户指定的协议）
    this.protocol = detectProtocol(opts.baseUrl, opts.protocol);

    // 初始化容灾客户端
    this.failoverClient = new FailoverClient({
      primary: { id: "primary", baseUrl: opts.baseUrl, apiKey: opts.apiKey, model: opts.model },
      fallbacks: opts.fallbacks,
      logger: opts.failoverLogger,
    });
  }

  async *run(input: AgentRunInput): AsyncIterable<AgentStreamItem> {
    yield { type: "status", status: "running" };

    try {
      let content = input.content || input.text;

      // Preprocess: upload local videos to Moonshot
      const needsVideoUpload = Array.isArray(content) &&
        content.some((p: any) => p.type === "video_url" && p.video_url?.url?.startsWith("file://"));
      if (needsVideoUpload) {
        yield { type: "status", status: "uploading_video" as any };
        const profiles = this.failoverClient.getProfiles();
        const profile = profiles.find(p => p.id === "primary") || profiles[0];
        if (profile) {
          const result = await preprocessMultimodalContent(content, profile, this.opts.videoUploadConfig);
          content = result.content;
        }
      }

      const messages = buildMessages(this.opts.systemPrompt, content, input.history);

      // 使用容灾客户端发送请求
      const { response: res } = await this.failoverClient.fetchWithFailover({
        timeoutMs: this.opts.timeoutMs,
        buildRequest: (profile) => this.buildRequest(profile, messages),
      });

      if (!res.ok) {
        const text = await safeReadText(res);
        yield { type: "final", text: `模型调用失败（HTTP ${res.status}）：${text}` };
        yield { type: "status", status: "error" };
        return;
      }

      if (!this.opts.stream) {
        const json = (await res.json()) as JsonObject;
        const content = this.getNonStreamContent(json);
        yield* emitChunkedFinal(content);
        return;
      }

      const body = res.body;
      if (!body) {
        yield { type: "final", text: "模型调用失败：响应体为空" };
        yield { type: "status", status: "error" };
        return;
      }

      let out = "";
      for await (const item of parseSseStream(body as any, this.protocol)) {
        if (item.type === "delta") {
          out += item.delta;
          yield item;
        }
        if (item.type === "final") {
          yield { type: "final", text: out };
          yield { type: "status", status: "done" };
          return;
        }
        if (item.type === "error") {
          yield { type: "final", text: item.message };
          yield { type: "status", status: "error" };
          return;
        }
      }

      yield { type: "final", text: out };
      yield { type: "status", status: "done" };
    } catch (err) {
      const msg = err instanceof Error ? err.message : String(err);
      yield { type: "final", text: `模型调用异常：${msg}` };
      yield { type: "status", status: "error" };
    }
  }

  private buildRequest(
    profile: { baseUrl: string; apiKey: string; model: string },
    messages: Array<{ role: string; content: any }>
  ): { url: string; init: RequestInit } {
    if (this.protocol === "anthropic") {
      // Anthropic 协议：提取 system 消息
      const systemMessage = messages.find(m => m.role === "system")?.content;
      const chatMessages = messages.filter(m => m.role !== "system");

      const payload: any = {
        model: profile.model,
        messages: chatMessages,
        max_tokens: 4096,
        stream: this.opts.stream,
      };

      if (systemMessage) {
        payload.system = systemMessage;
      }

      // 标准 Anthropic API headers
      // 注意：某些服务（如 Kimi Code）可能有额外的客户端校验
      const headers: Record<string, string> = {
        "content-type": "application/json",
        "x-api-key": profile.apiKey,
        "anthropic-version": "2023-06-01",
      };

      return {
        url: buildUrl(profile.baseUrl, "/v1/messages"),
        init: {
          method: "POST",
          headers,
          body: JSON.stringify(payload),
        },
      };
    }

    // OpenAI 协议
    const payload = {
      model: profile.model,
      messages,
      stream: this.opts.stream,
    };

    return {
      url: buildUrl(profile.baseUrl, "/chat/completions"),
      init: {
        method: "POST",
        headers: {
          "content-type": "application/json",
          authorization: `Bearer ${profile.apiKey}`,
        },
        body: JSON.stringify(payload),
      },
    };
  }

  private getNonStreamContent(json: JsonObject): string {
    if (this.protocol === "anthropic") {
      // Anthropic 格式：{ content: [{ type: "text", text: "..." }] }
      const content = (json.content as unknown) as Array<any> | undefined;
      return content?.[0]?.text ?? "";
    }

    // OpenAI 格式：{ choices: [{ message: { content: "..." } }] }
    const choices = (json.choices as unknown) as Array<any> | undefined;
    return choices?.[0]?.message?.content ?? "";
  }
}


function buildMessages(
  systemPrompt: string | undefined,
  userContent: string | Array<any>,
  history?: Array<{ role: "user" | "assistant"; content: string | Array<any> }>,
) {
  const messages: Array<{ role: "system" | "user" | "assistant"; content: any }> = [];

  // Layer 1: System
  if (systemPrompt && systemPrompt.trim()) {
    messages.push({ role: "system", content: systemPrompt.trim() });
  }

  // Layer 2: History
  if (history && history.length > 0) {
    messages.push(...history);
  }

  // Layer 3: Current User Message
  messages.push({ role: "user", content: userContent });

  return messages;
}

async function safeReadText(res: Response): Promise<string> {
  try {
    const text = await res.text();
    return text.length > 500 ? `${text.slice(0, 500)}…` : text;
  } catch {
    return "";
  }
}

async function* emitChunkedFinal(text: string): AsyncIterable<AgentStreamItem> {
  const chunks = splitText(text, 16);
  let out = "";
  for (const delta of chunks) {
    out += delta;
    yield { type: "delta", delta };
  }
  yield { type: "final", text: out };
  yield { type: "status", status: "done" };
}

function splitText(text: string, size: number): string[] {
  const out: string[] = [];
  let i = 0;
  while (i < text.length) {
    out.push(text.slice(i, i + Math.max(1, size)));
    i += Math.max(1, size);
  }
  return out;
}

type ParsedSseItem =
  | { type: "delta"; delta: string }
  | { type: "final" }
  | { type: "error"; message: string };

async function* parseSseStream(
  body: ReadableStream<Uint8Array>,
  protocol: ApiProtocol
): AsyncIterable<ParsedSseItem> {
  const reader = body.getReader();
  const decoder = new TextDecoder("utf-8");
  let buffer = "";

  while (true) {
    const { value, done } = await reader.read();
    if (done) break;
    buffer += decoder.decode(value, { stream: true });

    while (true) {
      const idx = buffer.indexOf("\n\n");
      if (idx < 0) break;
      const eventBlock = buffer.slice(0, idx);
      buffer = buffer.slice(idx + 2);

      const dataLines = eventBlock
        .split("\n")
        .map((l) => l.trim())
        .filter((l) => l.startsWith("data:"))
        .map((l) => l.slice("data:".length).trim());

      for (const data of dataLines) {
        if (data === "[DONE]") {
          yield { type: "final" };
          return;
        }
        try {
          const json = JSON.parse(data) as any;

          if (protocol === "anthropic") {
            // Anthropic SSE 格式
            // { type: "content_block_delta", delta: { type: "text_delta", text: "..." } }
            if (json.type === "content_block_delta" && json.delta?.text) {
              yield { type: "delta", delta: json.delta.text };
            }
            // 消息结束标记
            if (json.type === "message_stop") {
              yield { type: "final" };
              return;
            }
          } else {
            // OpenAI SSE 格式
            const delta = json?.choices?.[0]?.delta?.content;
            if (typeof delta === "string" && delta.length) {
              yield { type: "delta", delta };
            }
          }
        } catch {
          yield { type: "error", message: "模型流解析失败" };
          return;
        }
      }
    }
  }
}
