
### 总览：他们(UI-TARS-desktop-main)怎么做“看得准 + 点得准”

在 UI‑TARS 里，“屏幕截图和视觉识别支持”由 **`GUIAgent + Operator + VLM 模型`** 这一套链路完成，“精准鼠标键盘控制”则由 **本地 NutJS + 远程 RemoteComputer API** 两条链路实现。下面分块拆开说。

---

## 一、屏幕截图与视觉识别支持

### 1. Operator 侧的截图实现（本地 + 远程）

#### 1）本地桌面：NutJSOperator（通用版）

文件：`packages/ui-tars/operators/nut-js/src/index.ts`

- 使用 `nut-js` 的 `screen.grab()` 直接抓整屏，并拿到 **物理像素 + DPI 缩放信息**：  
  ```52:75:e:\project\belldandy\UI-TARS-desktop-main\packages\ui-tars\operators\nut-js\src\index.ts
  public async screenshot(): Promise<ScreenshotOutput> {
    const { logger } = useContext();
    const grabImage = await screen.grab();
    const screenWithScale = await grabImage.toRGB(); // widthScale = screenWidth * scaleX

    const scaleFactor = screenWithScale.pixelDensity.scaleX;
    // ...
    const width = screenWithScale.width / screenWithScale.pixelDensity.scaleX;
    const height = screenWithScale.height / screenWithScale.pixelDensity.scaleY;
  ```
- 用 `Jimp` 把带 DPI 缩放的截图转换成 **物理分辨率**：  
  ```67:81:e:\project\belldandy\UI-TARS-desktop-main\packages\ui-tars\operators\nut-js\src\index.ts
  const screenWithScaleImage = await Jimp.fromBitmap({
    width: screenWithScale.width,
    height: screenWithScale.height,
    data: Buffer.from(screenWithScale.data),
  });

  const physicalScreenImage = await screenWithScaleImage
    .resize({ w: width, h: height })
    .getBuffer('image/png'); // Use png format to avoid compression
  ```
- 输出给 Agent 的是：  
  - `base64`: PNG 的 base64  
  - `scaleFactor`: 显示器的 DPR（例如 2.0）  
  ```83:90:e:\project\belldandy\UI-TARS-desktop-main\packages\ui-tars\operators\nut-js\src\index.ts
  const output = {
    base64: physicalScreenImage.toString('base64'),
    scaleFactor,
  };
  ```

**意义**：保证模型看到的截图坐标和后续鼠标坐标都基于真实物理分辨率（而不是 OS Logical 坐标），是后面“点得准”的基础。

#### 2）Electron 桌面：NutJSElectronOperator（为桌面应用做的特化）

文件：`apps/ui-tars/src/main/agent/operator.ts`

- 继承 `NutJSOperator`，重写 `screenshot()`，使用 Electron 自带的 `desktopCapturer` 直接拿指定显示器：  
  ```36:83:e:\project\belldandy\UI-TARS-desktop-main\apps\ui-tars\src\main\agent\operator.ts
  public async screenshot(): Promise<ScreenshotOutput> {
    const { physicalSize, logicalSize, scaleFactor, id: primaryDisplayId } = getScreenSize();
    // ...
    const sources = await desktopCapturer.getSources({
      types: ['screen'],
      thumbnailSize: {
        width: Math.round(logicalSize.width),
        height: Math.round(logicalSize.height),
      },
    });
    const primarySource =
      sources.find((source) => source.display_id === primaryDisplayId.toString()) || sources[0];
    // ...
    const resized = screenshot.resize({
      width: physicalSize.width,
      height: physicalSize.height,
    });

    return {
      base64: resized.toJPEG(75).toString('base64'),
      scaleFactor,
    };
  }
  ```
- 关键点：  
  - 用 `screen.getPrimaryDisplay()` + `display_id` 精确选中主显示器，适配多屏。  
  - `thumbnailSize` 用逻辑尺寸，之后再 resize 到 `physicalSize`，结合 `scaleFactor`，**消除缩放带来的偏差**。  
  - 如果找不到合适的 source，会回退到 `super.screenshot()`（NutJS 方案），保证鲁棒性。

#### 3）远程电脑：RemoteComputerOperator 的截图

文件：`apps/ui-tars/src/main/remote/operators.ts`

- 借助远程代理（`RemoteComputer` / `SubsRemoteComputer`）获取远程屏幕尺寸和截图：  
  ```81:99:e:\project\belldandy\UI-TARS-desktop-main\apps\ui-tars\src\main\remote\operators.ts
  public async screenshot(): Promise<ScreenshotOutput> {
    const { width: physicalWidth, height: physicalHeight } =
      await this.remoteComputer.getScreenSize();
    // ...
    const base64 = await this.remoteComputer.takeScreenshot();

    return {
      base64: base64,
      scaleFactor: 1,
    };
  }
  ```
- 这里 scaleFactor 直接给 1，远端自己保证截图分辨率和坐标的一致性。

#### 4）主进程权限支持

文件：`apps/ui-tars/src/main/main.ts`

- 初始化时：  
  - 开启辅助功能（对 macOS 做额外权限请求）。  
  - 注册 `setDisplayMediaRequestHandler`，让前端（渲染进程）也可以通过 Web API 捕获屏幕（用于录制 UI、投屏等）。  
  ```74:83:e:\project\belldandy\UI-TARS-desktop-main\apps\ui-tars\src\main\main.ts
  const initializeApp = async () => {
    const isAccessibilityEnabled = app.isAccessibilitySupportEnabled();
    // ...
    if (env.isMacOS) {
      app.setAccessibilitySupportEnabled(true);
      const { ensurePermissions } = await import('@main/utils/systemPermissions');
      const ensureScreenCapturePermission = ensurePermissions();
    }
    // ...
  }
  ```

### 2. 视觉识别：截图如何喂给模型、并转成动作

核心在 `GUIAgent`（`packages/ui-tars/sdk/src/GUIAgent.ts`）：

1. **每一轮循环先截图**：  
   ```185:189:e:\project\belldandy\UI-TARS-desktop-main\packages\ui-tars\sdk\src\GUIAgent.ts
   const snapshot = await asyncRetry(() => operator.screenshot(), { ... });
   ```
2. 用 `Jimp` 读取 base64，得到 **宽高和 MIME**：  
   ```191:200:e:\project\belldandy\UI-TARS-desktop-main\packages\ui-tars\sdk\src\GUIAgent.ts
   const { width, height, mime } = await Jimp.fromBuffer(
     Buffer.from(replaceBase64Prefix(snapshot.base64), 'base64'),
   ).catch(() => ({ width: null, height: null, mime: '' }));
   ```
3. 把截图作为一条 “带 screenshotContext 的对话” 追加到 `conversations`：  
   ```213:225:e:\project\belldandy\UI-TARS-desktop-main\packages\ui-tars\sdk\src\GUIAgent.ts
   data.conversations.push({
     from: 'human',
     value: IMAGE_PLACEHOLDER,
     screenshotBase64: snapshot.base64,
     screenshotContext: {
       size: { width, height },
       mime,
       scaleFactor: snapshot.scaleFactor,
     },
     // ...
   });
   ```
4. 把历史对话 + 截图序列 + `systemPrompt`（包含 ACTION_SPACES）一起喂给 UI‑TARS 模型：  
   ```241:255:e:\project\belldandy\UI-TARS-desktop-main\packages\ui-tars\sdk\src\GUIAgent.ts
   const modelFormat = toVlmModelFormat({ historyMessages, conversations, systemPrompt });
   const vlmParams: InvokeParams = {
     ...processVlmParams(modelFormat.conversations, modelFormat.images),
     screenContext: { width, height },
     scaleFactor: snapshot.scaleFactor,
     // ...
   };
   const { prediction, parsedPredictions, ... } = await model.invoke(vlmParams);
   ```
5. **视觉识别的结果**是 `parsedPredictions`：模型输出解析成结构化动作：  
   - `action_type`: 如 `click`, `drag`, `type`, `scroll`, `hotkey`, `finished` …  
   - `action_inputs`: 包含 `start_box`, `end_box`, `content`, `direction`, `key` 等。  
6. 为每轮模型响应添加 `screenshotContext` 和 `predictionParsed`：  
   ```327:343:e:\project\belldandy\UI-TARS-desktop-main\packages\ui-tars\sdk\src\GUIAgent.ts
   data.conversations.push({
     from: 'gpt',
     value: predictionSummary,
     screenshotContext: {
       size: { width, height },
       scaleFactor: snapshot.scaleFactor,
     },
     predictionParsed: parsedPredictions,
   });
   ```

**总结**：  
- 视觉识别本身是 **多模态 LLM 做的**：模型从截图里“看 UI、看元素”，再根据 `ACTION_SPACES` 输出动作字符串。  
- SDK 这边负责：  
  - 把截图规范化（分辨率 + scaleFactor）；  
  - 提供 `screenContext` 给模型；  
  - 把文本 prediction 解析到 `parsedPredictions`（内部工具）；  
  - 带着 `screenWidth`, `screenHeight`, `scaleFactor` 传给 Operator 执行动作。

### 3. 视觉标记与点击位置高亮

#### 1）在截图上画点：`markClickPosition`

文件：`apps/ui-tars/src/main/utils/image.ts`

- 用 `setOfMarksOverlays` 计算每个预测动作对应的 SVG overlay 位置（逻辑坐标 + 偏移），再用 `sharp` 合成到原始截图：  
  ```13:27:e:\project\belldandy\UI-TARS-desktop-main\apps\ui-tars\src\main\utils\image.ts
  const { overlays = [] } = setOfMarksOverlays({
    predictions: data.parsed,
    screenshotContext: data.screenshotContext,
  });
  const imageOverlays: sharp.OverlayOptions[] = overlays
    .map((o) => {
      if (o.yPos && o.xPos) {
        return {
          input: Buffer.from(o.svg),
          top: o.yPos + o.offsetY,
          left: o.xPos + o.offsetX,
        };
      }
      return null;
    })
  ```
- 这样可以在回放或分享时看到“模型刚才想点哪里”。

#### 2）在屏幕上画点：`ScreenMarker`

文件：`apps/ui-tars/src/main/window/ScreenMarker.ts`

- 创建透明置顶 `BrowserWindow`，根据 `screenshotContext.scaleFactor` 把逻辑坐标变成屏幕像素：  
  ```201:235:e:\project\belldandy\UI-TARS-desktop-main\apps\ui-tars\src\main\window\ScreenMarker.ts
  const { overlays } = setOfMarksOverlays({ predictions, screenshotContext, ... });
  const { scaleFactor = 1 } = screenshotContext;
  // ...
  this.currentOverlay = new BrowserWindow({
    width: overlay.boxWidth || 300,
    height: overlay.boxHeight || 100,
    transparent: true,
    // ...
    ...(overlay.xPos && overlay.yPos && {
      x: (overlay.xPos + overlay.offsetX) * scaleFactor,
      y: (overlay.yPos + overlay.offsetY) * scaleFactor,
    }),
  });
  this.currentOverlay.setIgnoreMouseEvents(true, { forward: true });
  ```
- 这就是 UI‑TARS 的“屏幕上出现蓝色小框/高亮光标”的来源，既帮助调试，也让用户看到视觉识别结果。

---

## 二、精准鼠标和键盘控制的实现

### 1. 坐标体系：从模型的 box 到屏幕像素

所有 Operator 的 `execute()` 都依赖同一个工具：`parseBoxToScreenCoords`（在 `@ui-tars/sdk/core` 中），调用方式：

```101:113:e:\project\belldandy\UI-TARS-desktop-main\apps\ui-tars\src\main\remote\operators.ts
const { x: rawX, y: rawY } = parseBoxToScreenCoords({
  boxStr: startBoxStr,
  screenWidth,
  screenHeight,
});
```

- 输入：  
  - `boxStr`: 模型传回的 `"[x1, y1, x2, y2]"` 字符串（一般是归一化或 UI 坐标）。  
  - `screenWidth`, `screenHeight`: GUIAgent 根据截图真实宽高传入。  
  - `scaleFactor`: 由 screenshot 输出，用于从逻辑坐标映射到物理像素（内部会用到）。  
- 输出：`x, y` 是实际屏幕坐标，再被 `Math.round` 成整数。

**结论**：  
- “看”和“点”之间，中间经过一个统一的 **box → 像素** 变换函数，保证本地 / 远程 都有一致的坐标行为。

### 2. 本地 NutJSOperator：鼠标动作

文件：`packages/ui-tars/operators/nut-js/src/index.ts`

- 鼠标移动：  
  ```176:180:e:\project\belldandy\UI-TARS-desktop-main\packages\ui-tars\operators\nut-js\src\index.ts
  case 'mouse_move':
  case 'hover':
    await moveStraightTo(startX, startY);
  ```
  - `moveStraightTo` 用 `mouse.move(straightTo(new Point(startX, startY)))`，路径平滑。  
  - `mouse.config.mouseSpeed = 3600`，速度较高，但仍受库插值控制。

- 左键单击 / 双击 / 右键 / 中键：  
  ```182:211:e:\project\belldandy\UI-TARS-desktop-main\packages\ui-tars\operators\nut-js\src\index.ts
  case 'click':
  case 'left_click':
    await moveStraightTo(startX, startY);
    await sleep(100);
    await mouse.click(Button.LEFT);
  // ...
  case 'left_double':
    await moveStraightTo(startX, startY);
    await sleep(100);
    await mouse.doubleClick(Button.LEFT);
  // ...
  case 'right_click':
    await moveStraightTo(startX, startY);
    await sleep(100);
    await mouse.click(Button.RIGHT);
  ```
  - 先移动，后 sleep 100ms，再点击，避免 DPI / 快速移动导致偏差。

- 拖拽 / 选择：  
  ```213:232:e:\project\belldandy\UI-TARS-desktop-main\packages\ui-tars\operators\nut-js\src\index.ts
  case 'drag':
  case 'select': {
    if (action_inputs?.end_box) {
      const { x: endX, y: endY } = parseBoxToScreenCoords({ ... });
      if (startX && startY && endX && endY) {
        await moveStraightTo(startX, startY);
        await sleep(100);
        await mouse.drag(straightTo(new Point(endX, endY)));
      }
    }
  }
  ```
  - 用 `mouse.drag(straightTo(...))`，拖拽路径也是线性的，保证“拉框”准确。

- 滚动：  
  ```293:311:e:\project\belldandy\UI-TARS-desktop-main\packages\ui-tars\operators\nut-js\src\index.ts
  case 'scroll': {
    if (startX !== null && startY !== null) {
      await moveStraightTo(startX, startY);
    }
    switch (direction?.toLowerCase()) {
      case 'up': await mouse.scrollUp(5 * 100); break;
      case 'down': await mouse.scrollDown(5 * 100); break;
      // ...
    }
  }
  ```

### 3. 本地 NutJSOperator：键盘输入和热键

- 普通输入：  
  ```237:253:e:\project\belldandy\UI-TARS-desktop-main\packages\ui-tars\operators\nut-js\src\index.ts
  case 'type': {
    const content = action_inputs.content?.trim();
    if (content) {
      const stripContent = content.replace(/\\n$/, '').replace(/\n$/, '');
      keyboard.config.autoDelayMs = 0;
      if (process.platform === 'win32') {
        const originalClipboard = await clipboard.getContent();
        await clipboard.setContent(stripContent);
        await keyboard.pressKey(Key.LeftControl, Key.V);
        // ...
        await clipboard.setContent(originalClipboard);
      } else {
        await keyboard.type(stripContent);
      }

      if (content.endsWith('\n') || content.endsWith('\\n')) {
        await keyboard.pressKey(Key.Enter);
        await keyboard.releaseKey(Key.Enter);
      }

      keyboard.config.autoDelayMs = 500;
    }
  }
  ```
  - Windows 下用剪贴板 + `Ctrl+V`，减少 IME/延迟问题，提高稳定性；  
  - 结尾的 `\n` 单独用 Enter 实现，避免多输一个空行。

- 热键解析：  
  ```120:168:e:\project\belldandy\UI-TARS-desktop-main\packages\ui-tars\operators\nut-js\src\index.ts
  const keyMap = { return: Key.Enter, ctrl: platformCtrlKey, ... }
  const lowercaseKeyMap = Object.fromEntries(
    Object.entries(Key).map(([k, v]) => [k.toLowerCase(), v]),
  );
  const keys = keyStr
    .split(/[\s+]/)
    .map((k) => k.toLowerCase())
    .map((k) => keyMap[k] ?? lowercaseKeyMap[k])
    .filter(Boolean);
  ```
  - 支持 `"cmd" | "win" | "page down" | "arrowup"` 等各种写法；  
  - 自动做大小写归一、平台映射（Ctrl ↔ Cmd），保证跨平台一致。

### 4. Electron 特化：NutJSElectronOperator 的精细输入

文件：`apps/ui-tars/src/main/agent/operator.ts`

- 再次针对 Windows 做了 `type` 的剪贴板优化（使用 Electron 自己的 `clipboard` + `nut-js` 键盘）：  
  ```86:103:e:\project\belldandy\UI-TARS-desktop-main\apps\ui-tars\src\main\agent\operator.ts
  async execute(params: ExecuteParams): Promise<ExecuteOutput> {
    const { action_type, action_inputs } = params.parsedPrediction;

    if (action_type === 'type' && env.isWindows && action_inputs?.content) {
      const content = action_inputs.content?.trim();
      const stripContent = content.replace(/\\n$/, '').replace(/\n$/, '');
      const originalClipboard = clipboard.readText();
      clipboard.writeText(stripContent);
      await keyboard.pressKey(Key.LeftControl, Key.V);
      // ...
      clipboard.writeText(originalClipboard);
    } else {
      return await super.execute(params);
    }
  }
  ```

### 5. 远程电脑：RemoteComputerOperator 的精准动作

文件：`apps/ui-tars/src/main/remote/operators.ts`

- 和 NutJSOperator 一样，先通过 `parseBoxToScreenCoords` 把 `start_box` / `end_box` 转成屏幕坐标：  
  ```101:116:e:\project\belldandy\UI-TARS-desktop-main\apps\ui-tars\src\main\remote\operators.ts
  const { parsedPrediction, screenWidth, screenHeight, scaleFactor } = params;
  const { action_type, action_inputs } = parsedPrediction;
  const startBoxStr = action_inputs?.start_box || '';
  const { x: rawX, y: rawY } = parseBoxToScreenCoords({
    boxStr: startBoxStr,
    screenWidth,
    screenHeight,
  });
  const startX = rawX !== null ? Math.round(rawX) : null;
  const startY = rawY !== null ? Math.round(rawY) : null;
  ```
- 然后调用远端 API：  
  - `moveMouse`, `clickMouse`, `dragMouse`, `scroll`, `typeText`, `pressKey` 等。  
  - 同样有 hotkey 字符串解析函数 `getHotkeys`，兼容 `"cmd" | "page down" | "arrowleft"`。

**特点**：  
- 本地/远程都统一用 “**box → 像素**” 的转换逻辑；  
- 远程所有动作都在服务端执行，桌面端只负责坐标计算和日志，用于回放和调试。

---

## 小结：他们是怎样保证“看得准 + 点得准”的

- **看得准（视觉链路）**：  
  - Operator 做 DPI/多屏处理，输出 **物理分辨率 + scaleFactor** 的截图。  
  - GUIAgent 用 Jimp 校验尺寸，构造 `screenContext` 和 `screenshotContext`，给多模态模型提供足够的视觉上下文。  
  - 模型根据 `MANUAL.ACTION_SPACES` 输出结构化动作（含 box），SDK 负责解析。  
  - `markClickPosition` + `ScreenMarker` 将模型的视觉判断以 overlay 形式可视化出来，便于人类理解和调试。

- **点得准（控制链路）**：  
  - 统一用 `parseBoxToScreenCoords` 把 box（归一化/逻辑坐标）转成实际像素坐标，兼容本地/远程。  
  - 本地用 `nut-js` 控制鼠标/键盘，增加移动前后 `sleep`、统一鼠标速度、拖拽路径插值，保证精度和稳定性。  
  - Windows 上针对输入使用剪贴板 + 快捷键，减少 IME/延迟；热键字符串做平台和大小写映射。  
  - 远程通过 `RemoteComputer` API 做同样的动作，只是执行端换成远端机器。

